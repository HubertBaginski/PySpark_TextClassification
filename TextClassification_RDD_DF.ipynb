{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF, CountVectorizer, CountVectorizerModel \n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "import numpy as np\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# create spark context\n",
    "sc = SparkContext()\n",
    "#create spark session\n",
    "spark = SparkSession(sc)\n",
    "# save datapath in variable\n",
    "dataPath = \"hdfs:///path-to-file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {15069: 1.9497, 28433: 8.0562, 34116: 4.5124, 59645: 3.7441, 70882: 1.6007, 140853: 8.2794, 151357: 1.6096, 154253: 0.7359, 162236: 4.1442, 163495: 4.5189, 178046: 2.3898, 184663: 4.7604, 197139: 1.8032, 200484: 4.4134, 214848: 3.9753, 238153: 0.7052, 268040: 2.3346, 273004: 5.5225, 276491: 0.6626, 291135: 2.5324, 298766: 8.9725, 302147: 1.9417, 348943: 0.9745, 357784: 0.5969, 407524: 4.071, 431618: 4.85, 438276: 1.0532, 452367: 1.6265, 458110: 3.763, 461228: 5.403, 463522: 0.4456, 464125: 8.0562, 465091: 3.1995, 472985: 1.7347, 498665: 7.5862, 511771: 2.8014, 540404: 2.9936, 551905: 5.3752, 569592: 5.5548, 648331: 0.9585, 675737: 8.9678, 676489: 0.9381, 685921: 8.5671, 702250: 1.6272, 706364: 1.0889, 721336: 1.7269, 732481: 6.5746, 745992: 5.0807, 749264: 2.1061, 778868: 3.7964, 782550: 5.6952, 794488: 1.2825, 800830: 4.9383, 868014: 2.5599, 897662: 4.1726, 898785: 5.7144, 924138: 5.3219, 929834: 1.2924, 935701: 1.0537, 941745: 8.0562, 992550: 1.9337, 1003235: 4.1402, 1015434: 3.1061, 1017725: 3.2601, 1032572: 6.421})]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################################################################################################\n",
    "####### PART 1                 RDD                                            ##############################################################\n",
    "##########################################################################################################################################\n",
    "# RDD specific imports\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.feature import StandardScaler, StandardScalerModel, Normalizer\n",
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "import json\n",
    "# create rdd\n",
    "documents = sc.textFile(dataPath)\n",
    "# load as json\n",
    "dataset = documents.map(json.loads)\n",
    "# select only reviewteext\n",
    "reviews = dataset.map(lambda e: e['reviewText'] )\n",
    "# define stopwords\n",
    "stopwords = [\"i\", \"a\", \"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"just\",\"keep\",\"keeps\",\"kept\",\"know\",\"knows\",\"known\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"que\",\"quite\",\"qv\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"zero\"]\n",
    "# define function to lowercase and remove special chars\n",
    "def lower(lines):\n",
    "    punc='!\"#$%&\\'()*+,-./:;1234567890<=>?@[\\\\]^_`{|}~'\n",
    "    for ch in punc:\n",
    "        lines = lines.replace(ch, '')\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split()\n",
    "    return lines\n",
    "\n",
    "# lowercase and remove special chars\n",
    "tokens = reviews.map(lower)\n",
    "#remove stopwords\n",
    "tokens = tokens.filter(lambda x : x not in stopwords)\n",
    "#define hashing \n",
    "hashingTF = HashingTF()\n",
    "# get term frequency\n",
    "tf = hashingTF.transform(tokens)\n",
    "# define idf\n",
    "idf = IDF().fit(tf)\n",
    "# calcualte tfidf\n",
    "tfidf = idf.transform(tf)\n",
    "#return vlaues for review 1\n",
    "tfidf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################\n",
    "####### PART 2                 DF Pipeline                                             ##############################################################\n",
    "##########################################################################################################################################\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover, CountVectorizer, ChiSqSelector, StringIndexer, Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "df1 = spark.read.json(dataPath).select(\"reviewText\", \"category\")\n",
    "# tokenize and remove chars with lenght 1\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"tokenizedReview\", pattern=\"\\\\W+\").setMinTokenLength(2)\n",
    "#tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"tokenizedReview\")\n",
    "# remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"tokenizedReview\", outputCol=\"tokens\")\n",
    "# calculate term frequency as vector(word_id:tf)\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"tf\", minDF=2.0)\n",
    "# weight down tf by idf\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "# convert category to float and name it label\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "# calculate top 4000 features based on chisquared values and categories\n",
    "selector = ChiSqSelector(numTopFeatures=4000, featuresCol=\"tfidf\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "# normlaize results with l2 norm\n",
    "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"features\", p=2.0)\n",
    "# build pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, remover, cv, idf , indexer, selector, normalizer ])\n",
    "# run df1 through pipeline\n",
    "top_k_features = pipeline.fit(df1)\n",
    "#create final dataframe containign tfidf and chisq top features normalized => input for ML algo\n",
    "fin = top_k_features.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1782ac3d5e43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# save cv model to trasnform indices to words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcvModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# save chisq model to transformm incides to words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv' is not defined"
     ]
    }
   ],
   "source": [
    "# get top 4000 terms to the selectedFeatures indeces based on the small dataset\n",
    "# run manually through pipeline save models to values\n",
    "\n",
    "# save cv model to trasnform indices to words\n",
    "cvModel = cv.fit(df1)\n",
    "\n",
    "# save chisq model to transformm incides to words\n",
    "chiSqModel = selector.fit(df1)\n",
    "# transform indices to words\n",
    "terms = [cvModel.vocabulary[i] for i in chiSqModel.selectedFeatures]\n",
    "\n",
    "# write out to file \n",
    "with open('output_ds.txt', 'w') as f:\n",
    "    for item in sorted(terms):\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################\n",
    "####### PART 3           ML DF API                                                 ################################################################\n",
    "##########################################################################################################################################\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest, LinearSVCModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover, CountVectorizer, ChiSqSelector, StringIndexer, Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# set a seed for reproducability \n",
    "rnd = 30\n",
    "\n",
    "#read data\n",
    "df1 = spark.read.json(dataPath).select(\"reviewText\", \"category\")\n",
    "\n",
    "#split into train, test and validation data\n",
    "(train, validate, test) = df1.randomSplit([0.64, 0.16, 0.2], seed=rnd)        # train data was split 80-20 for train and validation\n",
    "\n",
    "\n",
    "# tokenize and remove chars with lenght 1\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"tokenizedReview\", pattern=\"\\\\W+\").setMinTokenLength(3)\n",
    "#tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"tokenizedReview\")\n",
    "# remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"tokenizedReview\", outputCol=\"tokens\")\n",
    "# calculate term frequency as vector(word_id:tf)\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"tf\", minDF=2.0)\n",
    "# weight down tf by idf\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "# convert category to float and name it label\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "# calculate top 4000 features based on chisquared values and categories\n",
    "selector = ChiSqSelector(numTopFeatures=4000, featuresCol=\"tfidf\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "# normlaize results with l2 norm\n",
    "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"features\", p=2.0)\n",
    "\n",
    "# extend the pipeline from 2\n",
    "#set the classifier to support vector machine\n",
    "classifier = LinearSVC()\n",
    "# use onevsrest classifier in order to handle multi class binary \n",
    "ml_classifier = OneVsRest(classifier=classifier)\n",
    "\n",
    "#extend and build pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, remover, cv, idf , indexer, selector, normalizer , ml_classifier ])\n",
    "\n",
    "# run data  through pipeline\n",
    "# create grid with svm parameters\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(classifier.maxIter, [1, 50, 100]) \\\n",
    "    .addGrid(classifier.regParam, [0.001, 0.01, 0.1]) \\\n",
    "    .addGrid(classifier.threshold, [0.00, 0.05, 0.1]) \\\n",
    "    .addGrid(classifier.aggregationDepth, [2, 5, 10]) \\\n",
    "    .addGrid(classifier.tol, [1e-6, 1e-4, 1e-5]) \\\n",
    "    .build()\n",
    "# create crossvalidation model with pipeline data, grid data and multiclassevaluator, with 6 folds evaluating 3 parallely\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"f1\"),\n",
    "    numFolds=6,\n",
    "    parallelism=3,\n",
    "    seed=rnd)\n",
    "\n",
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cvModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-646b48f8e4e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create prediction for validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvalidation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# create prediction for test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cvModel' is not defined"
     ]
    }
   ],
   "source": [
    "# create prediction for validation data\n",
    "validation = cvModel.transform(validate)\n",
    "# create prediction for test data\n",
    "prediction = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-825c21404f16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get correct predictions for validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcorrect_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# get number of wrong predictions for validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwrong_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# get correct predictions for test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'validation' is not defined"
     ]
    }
   ],
   "source": [
    "# get correct predictions for validation data\n",
    "correct_validation = validation.filter(validation.label == validation.prediction).count()\n",
    "# get number of wrong predictions for validation data\n",
    "wrong_validation = validation.filter(validation.label != validation.prediction).count()\n",
    "# get correct predictions for test data\n",
    "correct_test = prediction.filter(prediction.label == prediction.prediction).count()\n",
    "# get number of wrong predictions for test data\n",
    "wrong_test = prediction.filter(prediction.label != prediction.prediction).count()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for validation data\n",
    "# correct predictions 2118\n",
    "# wrong predictions 402\n",
    "# total number of predictions 2520 \n",
    "# Accuracy 84.047619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for test data\n",
    "# correct predictions 2653\n",
    "# wrong predictions 420\n",
    "# total number of predictions 3073 \n",
    "# Accuracy 86.33257"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
