{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF, CountVectorizer, CountVectorizerModel \n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "import numpy as np\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# create spark context\n",
    "sc = SparkContext()\n",
    "#create spark session\n",
    "spark = SparkSession(sc)\n",
    "# save datapath in variable\n",
    "dataPath = \"hdfs:///user/elmar/amazon-reviews/reviewscombined-small.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {15069: 1.9497, 28433: 8.0562, 34116: 4.5124, 59645: 3.7441, 70882: 1.6007, 140853: 8.2794, 151357: 1.6096, 154253: 0.7359, 162236: 4.1442, 163495: 4.5189, 178046: 2.3898, 184663: 4.7604, 197139: 1.8032, 200484: 4.4134, 214848: 3.9753, 238153: 0.7052, 268040: 2.3346, 273004: 5.5225, 276491: 0.6626, 291135: 2.5324, 298766: 8.9725, 302147: 1.9417, 348943: 0.9745, 357784: 0.5969, 407524: 4.071, 431618: 4.85, 438276: 1.0532, 452367: 1.6265, 458110: 3.763, 461228: 5.403, 463522: 0.4456, 464125: 8.0562, 465091: 3.1995, 472985: 1.7347, 498665: 7.5862, 511771: 2.8014, 540404: 2.9936, 551905: 5.3752, 569592: 5.5548, 648331: 0.9585, 675737: 8.9678, 676489: 0.9381, 685921: 8.5671, 702250: 1.6272, 706364: 1.0889, 721336: 1.7269, 732481: 6.5746, 745992: 5.0807, 749264: 2.1061, 778868: 3.7964, 782550: 5.6952, 794488: 1.2825, 800830: 4.9383, 868014: 2.5599, 897662: 4.1726, 898785: 5.7144, 924138: 5.3219, 929834: 1.2924, 935701: 1.0537, 941745: 8.0562, 992550: 1.9337, 1003235: 4.1402, 1015434: 3.1061, 1017725: 3.2601, 1032572: 6.421})]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################################################################################################\n",
    "####### PART 1                                                             ##############################################################\n",
    "##########################################################################################################################################\n",
    "# RDD specific imports\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.feature import StandardScaler, StandardScalerModel, Normalizer\n",
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "import json\n",
    "# create rdd\n",
    "documents = sc.textFile(dataPath)\n",
    "# load as json\n",
    "dataset = documents.map(json.loads)\n",
    "# select only reviewteext\n",
    "reviews = dataset.map(lambda e: e['reviewText'] )\n",
    "# define stopwords\n",
    "stopwords = [\"i\", \"a\", \"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"just\",\"keep\",\"keeps\",\"kept\",\"know\",\"knows\",\"known\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"que\",\"quite\",\"qv\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"zero\"]\n",
    "# define function to lowercase and remove special chars\n",
    "def lower(lines):\n",
    "    punc='!\"#$%&\\'()*+,-./:;1234567890<=>?@[\\\\]^_`{|}~'\n",
    "    for ch in punc:\n",
    "        lines = lines.replace(ch, '')\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split()\n",
    "    return lines\n",
    "\n",
    "# lowercase and remove special chars\n",
    "tokens = reviews.map(lower)\n",
    "#remove stopwords\n",
    "tokens = tokens.filter(lambda x : x not in stopwords)\n",
    "#define hashing \n",
    "hashingTF = HashingTF()\n",
    "# get term frequency\n",
    "tf = hashingTF.transform(tokens)\n",
    "# define idf\n",
    "idf = IDF().fit(tf)\n",
    "# calcualte tfidf\n",
    "tfidf = idf.transform(tf)\n",
    "#return vlaues for review 1\n",
    "tfidf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################\n",
    "####### PART 2                                                              ##############################################################\n",
    "##########################################################################################################################################\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover, CountVectorizer, ChiSqSelector, StringIndexer, Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "df1 = spark.read.json(dataPath).select(\"reviewText\", \"category\")\n",
    "# tokenize and remove chars with lenght 1\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"tokenizedReview\", pattern=\"\\\\W+\").setMinTokenLength(2)\n",
    "#tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"tokenizedReview\")\n",
    "# remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"tokenizedReview\", outputCol=\"tokens\")\n",
    "# calculate term frequency as vector(word_id:tf)\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"tf\", minDF=2.0)\n",
    "# weight down tf by idf\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "# convert category to float and name it label\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "# calculate top 4000 features based on chisquared values and categories\n",
    "selector = ChiSqSelector(numTopFeatures=4000, featuresCol=\"tfidf\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "# normlaize results with l2 norm\n",
    "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"features\", p=2.0)\n",
    "# build pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, remover, cv, idf , indexer, selector, normalizer ])\n",
    "# run df1 through pipeline\n",
    "top_k_features = pipeline.fit(df1)\n",
    "#create final dataframe containign tfidf and chisq top features normalized => input for ML algo\n",
    "fin = top_k_features.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tokens                                                                                                                                                                                                                                                                                                        |tfidf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[gift, husband, making, things, time, love, food, directions, simple, easy, read, interpret, fun, make, love, different, kinds, cuisine, raichlen, provides, recipes, everywhere, along, barbecue, trail, calls, get, open, page, love, food, provided, insight, culture, produced, broadening, horizons, yum]|(20678,[5,7,8,9,27,30,70,92,101,242,244,249,278,328,351,379,436,597,608,745,1123,1133,1317,1383,1501,1570,1924,3546,8424,11830,11833,12331,12959,18220],[1.909907257134484,5.656114017987326,1.9107641552160517,1.9315531239091768,2.5079389429875754,2.552532318530394,2.982313481471207,3.0892048581892575,3.1749508927349743,3.7414186298229497,3.7630410938361156,3.7202538186309067,3.958564162488606,3.9519416217281127,4.089725324091166,4.140221488105698,8.912376548792121,4.506619128022953,4.929475978842986,4.738420742080277,5.070554577102892,5.30896560054789,5.348186313701171,5.348186313701171,5.5067913438778096,5.522539700845949,5.714430708656054,6.446798602369281,7.586232885557646,8.05623651480338,8.05623651480338,8.05623651480338,8.05623651480338,8.567062138569373])|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get top 4000 terms to the selectedFeatures indeces based on the small dataset\n",
    "# run manually through pipeline save models to values\n",
    "#df1 = spark.read.json(dataPath).select(\"reviewText\", \"category\")\n",
    "#regexTokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"tokenizedReview\", pattern=\"\\\\W+\").setMinTokenLength(3)\n",
    "#df1 = regexTokenizer.transform(df1)\n",
    "#df1 = remover.transform(df1)\n",
    "# save cv model to trasnform indices to words\n",
    "cvModel = cv.fit(df1)\n",
    "#df1 = cvModel.transform(df1)\n",
    "#df1 = idf.fit(df1).transform(df1)\n",
    "#df1 = indexer.fit(df1).transform(df1)\n",
    "# save chisq model to transformm incides to words\n",
    "chiSqModel = selector.fit(df1)\n",
    "# transform indices to words\n",
    "terms = [cvModel.vocabulary[i] for i in chiSqModel.selectedFeatures]\n",
    "\n",
    "# write out to file \n",
    "with open('output_ds.txt', 'w') as f:\n",
    "    for item in sorted(terms):\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################\n",
    "####### PART 3                                                            ################################################################\n",
    "##########################################################################################################################################\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest, LinearSVCModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover, CountVectorizer, ChiSqSelector, StringIndexer, Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# set a seed for reproducability \n",
    "rnd = 30\n",
    "\n",
    "#read data\n",
    "df1 = spark.read.json(dataPath).select(\"reviewText\", \"category\")\n",
    "\n",
    "#split into train, test and validation data\n",
    "(train, validate, test) = df1.randomSplit([0.64, 0.16, 0.2], seed=rnd)        # train data was split 80-20 for train and validation\n",
    "\n",
    "\n",
    "# tokenize and remove chars with lenght 1\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"tokenizedReview\", pattern=\"\\\\W+\").setMinTokenLength(3)\n",
    "#tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"tokenizedReview\")\n",
    "# remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"tokenizedReview\", outputCol=\"tokens\")\n",
    "# calculate term frequency as vector(word_id:tf)\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"tf\", minDF=2.0)\n",
    "# weight down tf by idf\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "# convert category to float and name it label\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "# calculate top 4000 features based on chisquared values and categories\n",
    "selector = ChiSqSelector(numTopFeatures=4000, featuresCol=\"tfidf\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "# normlaize results with l2 norm\n",
    "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"features\", p=2.0)\n",
    "\n",
    "# extend the pipeline from 2\n",
    "#set the classifier to support vector machine\n",
    "classifier = LinearSVC()\n",
    "# use onevsrest classifier in order to handle multi class binary \n",
    "ml_classifier = OneVsRest(classifier=classifier)\n",
    "\n",
    "#extend and build pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, remover, cv, idf , indexer, selector, normalizer , ml_classifier ])\n",
    "\n",
    "# run data  through pipeline\n",
    "# create grid with svm parameters\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(classifier.maxIter, [1, 50, 100]) \\\n",
    "    .addGrid(classifier.regParam, [0.001, 0.01, 0.1]) \\\n",
    "    .addGrid(classifier.threshold, [0.00, 0.05, 0.1]) \\\n",
    "    .addGrid(classifier.aggregationDepth, [2, 5, 10]) \\\n",
    "    .addGrid(classifier.tol, [1e-6, 1e-4, 1e-5]) \\\n",
    "    .build()\n",
    "# create crossvalidation model with pipeline data, grid data and multiclassevaluator, with 6 folds evaluating 3 parallely\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"f1\"),\n",
    "    numFolds=6,\n",
    "    parallelism=3,\n",
    "    seed=rnd)\n",
    "\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "#cvModel.save(\"home/dic/2019S/users/e01551118g/cvModelv4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o62.load.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://c100.local:8020/user/e01551118g/home/dic/2019S/users/e01551118g/cvModelv4/metadata\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1337)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1331)\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1372)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1371)\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:387)\n\tat org.apache.spark.ml.tuning.ValidatorParams$.loadImpl(ValidatorParams.scala:180)\n\tat org.apache.spark.ml.tuning.CrossValidatorModel$CrossValidatorModelReader.load(CrossValidator.scala:388)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-436d0e8b1724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossValidatorModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcvModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossValidatorModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"home/dic/2019S/users/e01551118g/cvModelv4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clazz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_from_java\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             raise NotImplementedError(\"This Java ML type cannot be loaded into Python currently: %r\"\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o62.load.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://c100.local:8020/user/e01551118g/home/dic/2019S/users/e01551118g/cvModelv4/metadata\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1337)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1331)\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1372)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1371)\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:387)\n\tat org.apache.spark.ml.tuning.ValidatorParams$.loadImpl(ValidatorParams.scala:180)\n\tat org.apache.spark.ml.tuning.CrossValidatorModel$CrossValidatorModelReader.load(CrossValidator.scala:388)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.ml.tuning import CrossValidatorModel\n",
    "#cvModel = CrossValidatorModel.load(\"home/dic/2019S/users/e01551118g/cvModelv4\")\n",
    "\n",
    "# create prediction for validation data\n",
    "validation = cvModel.transform(validate)\n",
    "# create prediction for test data\n",
    "prediction = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correct predictions for validation data\n",
    "correct_validation = validation.filter(validation.label == validation.prediction).count()\n",
    "# get number of wrong predictions for validation data\n",
    "wrong_validation = validation.filter(validation.label != validation.prediction).count()\n",
    "# get correct predictions for test data\n",
    "correct_test = prediction.filter(prediction.label == prediction.prediction).count()\n",
    "# get number of wrong predictions for test data\n",
    "wrong_test = prediction.filter(prediction.label != prediction.prediction).count()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for validation data\n",
    "# correct predictions 2118\n",
    "# wrong predictions 402\n",
    "# total number of predictions 2520 \n",
    "# Accuracy 84.047619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for test data\n",
    "# correct predictions 2653\n",
    "# wrong predictions 420\n",
    "# total number of predictions 3073 \n",
    "# Accuracy 86.33257"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3 (Spark 2.3)",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
